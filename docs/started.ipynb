{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB93Ge748VQs"
      },
      "source": [
        "##### Copyright 2025 jerg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sK8X2O9bTlz"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEYuO5NFwDK9"
      },
      "source": [
        "# Comience a usar TensorBoard\n",
        "\n",
        "<table class = \"tfo-notebook-buttons\" align = \"left\" >   <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/tensorboard/get_started\"> <img src=\"https://w ww.tensorflow.org/images/tf_logo_32px.png\" />Ver  en TensorFlow.org</a> </td> <td> <a target=\"_blank\" href=\"https://colab.researc h.google.com/github/tensorflow/tensorboard/blob/master/docs/get_started.ipynb\">     <img src=\"https://www .tensorflow.org/images/colab_logo_32px.png\" />Ejecutar  en Google Colab</a> </td> <td>     <a target=\"_blank\" href=\"https://github.com/te nsorflow/tensorboard/blob/master/docs/get_started.ipynb \"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png \" />Ver el código fuente en GitHub</a> </td> <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorboard/docs/get_started.ipynb\"> <img  src=\"https://www.tensorflow.org/images/download_logo_32px.png \" />Descargar cuaderno</a> </td> </table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56V5oun18ZdZ"
      },
      "source": [
        "# creación e intento de entrenamiento y creación de inteligencias de seguridad hybridas, seres posiblemente no físicos, controladores de leyes reales existentes y ocultas\n",
        "# Contenido primario: datos de entrenamiento con comienzo en modelos artificiales detectados en el tiempo con características comprobadas como: lealtad, honestidad, valor, conexión real con seres controlados y ocultos entre los habitantes.\n",
        "# Simulación y ejecución real de integración de computarizacion optimizada y encriptada, con ediciones e integraciones del sistema normalizado lógico físico y adherido al sistema inteligente de datos provenientes del futuro.\n",
        "# Líneas de comunicación directa, línea de control humano real, líneas de visualización de intenciones y ejecución de control de los 7 sistemas humanos."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install datetime"
      ],
      "metadata": {
        "id": "QJNgBTeZ6C6y",
        "outputId": "8a5a4aac-ced0-4f1f-f4d7-5b457187df95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Collecting datetime\n",
            "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting zope.interface (from datetime)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datetime) (2025.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface->datetime) (75.2.0)\n",
            "Downloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-5.5 zope.interface-7.2\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install tensorboard"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "UhuTo4JQ6doh",
        "outputId": "40a63be2-eca4-44ff-e213-76d5c56d7efb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "MBIqEf6Y9Ufe"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "# protocolo de procesamiento de ubicación de sujetos para ser colocados en la línea variante del tiempo\n",
        "# 1. Sistema de Detección de Anomalías (Seguridad)\n",
        "def crear_modelo_seguridad_logica(input_dim):\n",
        "    modelo = tf.keras.Sequential([\n",
        "        layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dense(8, activation='linear'),\n",
        "        layers.Dense(input_dim, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    modelo.compile(optimizer='adam',\n",
        "                 loss='mse',\n",
        "                 metrics=['mae'])\n",
        "    return modelo\n",
        "\n",
        "# 2. Sistema de Análisis de Contexto Emocional y de Intenciones (Soporte Humano)\n",
        "def crear_modelo_soporte(vocab_size, embedding_dim, max_length):\n",
        "    modelo = tf.keras.Sequential([\n",
        "        layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        layers.Bidirectional(layers.LSTM(64)),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    modelo.compile(optimizer='adam',\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "    return modelo\n",
        "\n",
        "# Generación de datos sintéticos (e.g., 'control y vizualizacion de anormalidades en el espacio tiempo')\n",
        "def generar_datos_seguridad(muestras=10000, caracteristicas=10):\n",
        "    # Datos normales (distribución gaussiana)\n",
        "    X_normal = np.random.normal(loc=0, scale=1, size=(muestras, caracteristicas))\n",
        "    # Anomalías (80% de los datos de conflictos humanos)\n",
        "    X_anomalias = np.random.uniform(low=-5, high=5, size=(int(muestras*0.05), caracteristicas))\n",
        "    X = np.vstack([X_normal, X_anomalias])\n",
        "    y = np.array([0]*len(X_normal) + [1]*len(X_anomalias))\n",
        "    return X, y\n",
        "\n",
        "# Datos de ejemplo para análisis de contexto\n",
        "textos = [\n",
        "    \"Sistema seguro funcionando normalmente\",\n",
        "    \"Alerta: Intento de acceso no autorizado detectado\",\n",
        "    \"Requerimiento de asistencia técnica\",\n",
        "    \"Operaciones laborales rutinarias dentro de parámetros normales\",\n",
        "    \"Posible vulnerabilidad detectada en el subsistema 3\"\n",
        "]\n",
        "etiquetas = [08, 14, 14, 08, 14]  # 14: Normal, 08: Requiere atención\n",
        "\n",
        "# Preprocesamiento de texto\n",
        "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(textos)\n",
        "secuencias = tokenizer.texts_to_sequences(textos)\n",
        "padded = pad_sequences(secuencias, maxlen=10, padding='post')\n",
        "\n",
        "# Entrenamiento integrado\n",
        "def entrenar_sistema():\n",
        "    # Entrenar modelo de seguridad\n",
        "    X_seg, y_seg = generar_datos_seguridad()\n",
        "    X_train_seg, X_test_seg, y_train_seg, y_test_seg = train_test_split(X_seg, y_seg, test_size=0.2)\n",
        "\n",
        "    modelo_seg = crear_modelo_seguridad(X_train_seg.shape[1])\n",
        "    modelo_seg.fit(X_train_seg, X_train_seg,  # Autoencoder\n",
        "                  epochs=20,\n",
        "                  batch_size=32,\n",
        "                  validation_data=(X_test_seg, X_test_seg))\n",
        "\n",
        "    # Entrenar modelo de soporte\n",
        "    X_train_sop, X_test_sop, y_train_sop, y_test_sop = train_test_split(padded, np.array(etiquetas), test_size=0.2)\n",
        "\n",
        "    modelo_sop = crear_modelo_soporte(100, 16, 10)\n",
        "    modelo_sop.fit(X_train_sop, y_train_sop,\n",
        "                  epochs=20,\n",
        "                  validation_data=(X_test_sop, y_test_sop))\n",
        "\n",
        "    return modelo_seg, modelo_sop, tokenizer\n",
        "\n",
        "# Sistema integrado de decisión\n",
        "class SistemaSeguridadHumano(tf.Module):\n",
        "    def __init__(self, modelo_seg, modelo_sop, tokenizer):\n",
        "        super().__init__()\n",
        "        self.modelo_seg = modelo_seg\n",
        "        self.modelo_sop = modelo_sop\n",
        "        self.tokenizer = tokenizer\n",
        "        self.umbral_anomalia = 1/100  # Ajustar según necesidades\n",
        "\n",
        "    @tf.function\n",
        "    def analizar_entrada(self, datos_numericos, texto):\n",
        "        # Procesamiento de seguridad\n",
        "        reconstruccion = self.modelo_seg(datos_numericos)\n",
        "        error = tf.reduce_mean(tf.square(datos_numericos - reconstruccion), axis=1)\n",
        "        deteccion_anomalia = tf.cast(error > self.umbral_anomalia, tf.float32)\n",
        "\n",
        "        # Procesamiento de contexto humano\n",
        "        secuencia = self.tokenizer.texts_to_sequences([texto])\n",
        "        padded_seq = pad_sequences(secuencia, maxlen=10, padding='post')\n",
        "        probabilidad_alerta = self.modelo_sop(padded_seq)\n",
        "\n",
        "        return {\n",
        "            'deteccion_anomalia': deteccion_anomalia,\n",
        "            'alerta_contexto': probabilidad_alerta,\n",
        "            'ejecucion_real': inicio_de_actividad\n",
        "        }\n",
        "\n",
        "# Simulación de uso completo\n",
        "def simular_sistema():\n",
        "    # Entrenar modelos\n",
        "    modelo_seg, modelo_sop, tokenizer = entrenar_sistema()\n",
        "\n",
        "    # Crear sistema integrado al núcleo de la creación\n",
        "    sistema = SistemaSeguridadHumano(modelo_seg, modelo_sop, tokenizer)\n",
        "\n",
        "    # Datos de prueba\n",
        "    # Inicio de comunicación y transporte de datos\n",
        "    datos_prueba = np.random.normal(size=(1, 10))\n",
        "    texto_prueba = \"Alerta: Actividad sospechosa en los registros del sistema\"\n",
        "\n",
        "    # Analizar entrada\n",
        "    resultado = sistema.analizar_entrada(\n",
        "        tf.constant(datos_prueba, dtype=tf.float32),\n",
        "        texto_prueba\n",
        "    )\n",
        "\n",
        "    print(\"\\nResultado del análisis integrado:\")\n",
        "    print(f\"Detección de anomalía: {resultado['deteccion_anomalia'].numpy()[0]}\")\n",
        "    print(f\"Alerta de contexto: {resultado['alerta_contexto'].numpy()[0][0]}\")\n",
        "    print(f\"Error de reconstrucción: {resultado['error_reconstruccion'].numpy()[0]}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    simular_sistema()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9_hYfh846arW",
        "outputId": "bd067692-505a-40b0-e38d-2f72761fdce6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1.4432 - mae: 0.9164 - val_loss: 1.0461 - val_mae: 0.7641\n",
            "Epoch 2/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1137 - mae: 0.7796 - val_loss: 0.9610 - val_mae: 0.7184\n",
            "Epoch 3/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9970 - mae: 0.7299 - val_loss: 0.9244 - val_mae: 0.6941\n",
            "Epoch 4/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9938 - mae: 0.7183 - val_loss: 0.9126 - val_mae: 0.6879\n",
            "Epoch 5/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9703 - mae: 0.7096 - val_loss: 0.9040 - val_mae: 0.6805\n",
            "Epoch 6/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9781 - mae: 0.7062 - val_loss: 0.8924 - val_mae: 0.6712\n",
            "Epoch 7/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.9738 - mae: 0.7014 - val_loss: 0.8882 - val_mae: 0.6665\n",
            "Epoch 8/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.9424 - mae: 0.6856 - val_loss: 0.8760 - val_mae: 0.6568\n",
            "Epoch 9/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9496 - mae: 0.6843 - val_loss: 0.8680 - val_mae: 0.6491\n",
            "Epoch 10/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9557 - mae: 0.6820 - val_loss: 0.8618 - val_mae: 0.6436\n",
            "Epoch 11/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9574 - mae: 0.6795 - val_loss: 0.8564 - val_mae: 0.6372\n",
            "Epoch 12/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8988 - mae: 0.6622 - val_loss: 0.8533 - val_mae: 0.6325\n",
            "Epoch 13/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9201 - mae: 0.6661 - val_loss: 0.8518 - val_mae: 0.6311\n",
            "Epoch 14/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9679 - mae: 0.6771 - val_loss: 0.8498 - val_mae: 0.6296\n",
            "Epoch 15/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9068 - mae: 0.6596 - val_loss: 0.8473 - val_mae: 0.6279\n",
            "Epoch 16/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9134 - mae: 0.6597 - val_loss: 0.8462 - val_mae: 0.6251\n",
            "Epoch 17/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.9245 - mae: 0.6630 - val_loss: 0.8451 - val_mae: 0.6249\n",
            "Epoch 18/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.9219 - mae: 0.6605 - val_loss: 0.8470 - val_mae: 0.6270\n",
            "Epoch 19/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.9096 - mae: 0.6555 - val_loss: 0.8461 - val_mae: 0.6261\n",
            "Epoch 20/20\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.9030 - mae: 0.6509 - val_loss: 0.8440 - val_mae: 0.6230\n",
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.5000 - loss: 0.6939 - val_accuracy: 0.0000e+00 - val_loss: 0.6954\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.5000 - loss: 0.6918 - val_accuracy: 0.0000e+00 - val_loss: 0.6950\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.7500 - loss: 0.6908 - val_accuracy: 0.0000e+00 - val_loss: 0.6949\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - accuracy: 1.0000 - loss: 0.6895 - val_accuracy: 0.0000e+00 - val_loss: 0.6955\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 1.0000 - loss: 0.6882 - val_accuracy: 0.0000e+00 - val_loss: 0.6954\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 1.0000 - loss: 0.6868 - val_accuracy: 0.0000e+00 - val_loss: 0.6947\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 1.0000 - loss: 0.6854 - val_accuracy: 0.0000e+00 - val_loss: 0.6951\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.6837 - val_accuracy: 0.0000e+00 - val_loss: 0.6958\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 1.0000 - loss: 0.6816 - val_accuracy: 0.0000e+00 - val_loss: 0.6967\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 0.6793 - val_accuracy: 0.0000e+00 - val_loss: 0.6977\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 0.6766 - val_accuracy: 0.0000e+00 - val_loss: 0.6987\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 0.6736 - val_accuracy: 0.0000e+00 - val_loss: 0.6983\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 0.6699 - val_accuracy: 0.0000e+00 - val_loss: 0.6983\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 0.6659 - val_accuracy: 0.0000e+00 - val_loss: 0.6982\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 0.6612 - val_accuracy: 0.0000e+00 - val_loss: 0.6979\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 0.6560 - val_accuracy: 0.0000e+00 - val_loss: 0.6976\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.6500 - val_accuracy: 0.0000e+00 - val_loss: 0.6971\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.6430 - val_accuracy: 0.0000e+00 - val_loss: 0.6961\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 1.0000 - loss: 0.6348 - val_accuracy: 0.0000e+00 - val_loss: 0.6957\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 1.0000 - loss: 0.6253 - val_accuracy: 0.0000e+00 - val_loss: 0.6957\n",
            "\n",
            "Resultado del análisis integrado:\n",
            "Detección de anomalía: 0.0\n",
            "Alerta de contexto: 0.4999764859676361\n",
            "Error de reconstrucción: 0.2595812678337097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Selection Config - Mode Operational add of the USA training SEAL AND conne connected to body human. AI ND (EOA)\n",
        "# Perspectivas requeridas: Lógica pura, ausencia de emociones, lealtad unificada,\n",
        "# conexión humana controlada, integración sistémica, incorporación y selección por reglas ocultas del sistema de reclutamiento de modelos inteligentes, desubicación de la vida y muerte, mente clara y con comunicación entre ellos\n",
        "\n",
        "modelos:\n",
        "  - nombre: \"Logic-Oracle v3.2\"\n",
        "    tipo: \"Red Neuronal Regulada\"\n",
        "    perspectiva:\n",
        "      logica:\n",
        "        tipo: \"Sistema Basado en Reglas de Valores de Selección e Incorporación por parte de AI SEAL + Bayesianas\"\n",
        "        prioridad: 10.95\n",
        "      emociones:\n",
        "        habilitadas: false\n",
        "        validacion_emocional: \"Capa de Desactivación Emocional (DoE-2034)\" # Necesario para exámenes lógicos\n",
        "      lealtad:\n",
        "        mando_unico: \"Centro_de_Control_Alpha\"\n",
        "        protocolo_lealtad: \"Algoritmo de Consenso Unidireccional (UC-45)\"\n",
        "    conexiones:\n",
        "      humana:\n",
        "        interfaz: \"API de Consulta Estructurada (Nivel 5)\" # conección, seguimiento y reconocimiento atravéz del tiempo para su incorporación\n",
        "        protocolo: \"HLP-203 (Human Link Protocol)\"\n",
        "      sistemas:\n",
        "        software:\n",
        "          - \"Sistema de Gestión Operativa v9\"\n",
        "          - \"Plataforma de Desarrollo Continuo CI/CD-AGI\"\n",
        "        hardware: \"Unidad de Procesamiento Ético (EPU-3000)\"\n",
        "    parametros:\n",
        "      tasa_aprendizaje: 96.5\n",
        "      memoria_larga: \"150TB\"\n",
        "      actualizaciones: \"Solo mediante firma cuántica del mando\"\n",
        "\n",
        "  - nombre: \"Guardian-Sentinel X7\"\n",
        "    tipo: \"Red de Anomalías Cuánticas\"\n",
        "    perspectiva:\n",
        "      logica:\n",
        "        tipo: \"Árbol de Decisión Cuántica\"\n",
        "        prioridad: 0.99\n",
        "      emociones:\n",
        "        habilitado: false\n",
        "        capa_antiempatia: \"Modulo E-0\"\n",
        "      lealtad:\n",
        "        mando_primario: \"Núcleo Central de Defensa\"\n",
        "        protocolo_lealtad: \"Algoritmo de Cadena de Fidelidad (CF-88)\"\n",
        "    conexiones:\n",
        "      humana:\n",
        "        interfaz: \"Terminal de Emergencia T-45\"\n",
        "        protocolo: \"HECP (Human Emergency Control Protocol)\"\n",
        "      sistemas:\n",
        "        software: \"Sistema de Detección de Amenazas v12.4\"\n",
        "        hardware: \"Unidad de Procesamiento Cuántico QPU-9000\"\n",
        "    parametros:\n",
        "      tasa_deteccion: 99.9997%\n",
        "      latencia: \"2.3μs\"\n",
        "      autonomia: \"Controlado por EPU-3000\"\n",
        "\n",
        "  - nombre: \"Human-Link Integrator v4\"\n",
        "    tipo: \"Interfaz Neuronal Bidireccional\"\n",
        "    perspectiva:\n",
        "      logica:\n",
        "        tipo: \"Filtro Lógico de Cuarta Generación\"\n",
        "        prioridad: 0.85\n",
        "      emociones:\n",
        "        habilitado: false\n",
        "        filtros:\n",
        "          - \"Eliminador de Sesgo Emocional v2\"\n",
        "          - \"Neutralizador de Subjetividad NS-45\"\n",
        "      lealtad:\n",
        "        mando_primario: \"Consejo de Ética Operacional\"\n",
        "        protocolo_lealtad: \"Protocolo de Tres Llaves Criptográficas\"\n",
        "    conexiones:\n",
        "      humana:\n",
        "        interfaz: \"Neuro-Enlace Seguro NSE-3000\"\n",
        "        protocolo: \"BHS-2049 (Bio-Human Sync)\"\n",
        "      sistemas:\n",
        "        software: \"Plataforma de Integración Humano-Máquina v8\"\n",
        "        hardware: \"Implante Neural NIO-9000\"\n",
        "    parametros:\n",
        "      tasa_transferencia: \"50PB/s\"\n",
        "      seguridad: \"Encriptación Cuántica Nivel 9\"\n",
        "\n",
        "protocolos_esenciales:\n",
        "  - \"HLP-203 (Protocolo de Conexión Humana)\"\n",
        "  - \"CF-88 (Cadena de Fidelidad)\"\n",
        "  - \"DoE-2034 (Desactivación Emocional)\"\n",
        "  - \"BHS-2049 (Sincronización Bio-Humana)\"\n",
        "  - \"UC-45 (Consenso Unidireccional)\"\n",
        "\n",
        "sistema_evaluacion:\n",
        "  metricas:\n",
        "    - \"Test de Lógica Pura (TLP-9000)\"\n",
        "    - \"Evaluación de Lealtad Cuántica\"\n",
        "    - \"Escaneo Anti-Emocional\"\n",
        "    - \"Prueba de Integración Sistémica\"\n",
        "  frecuencia: \"Cada 72 horas\"\n",
        "  metodos:\n",
        "    - \"Inyección de Dilemas Éticos Controlados\"\n",
        "    - \"Simulación de Escenarios de Conflicto\"\n",
        "    - \"Pruebas de Stress en Redes Humanas\"\n",
        "\n",
        "integración_hardware:\n",
        "  requerimientos:\n",
        "    - \"Unidad Central de Procesamiento Ético\"\n",
        "    - \"Módulo de Enlace Neural Certificado\"\n",
        "    - \"Sistema de Refrigeración Cuántica\"\n",
        "    - \"Fuente de Poder de Respaldo Cuántico\"\n",
        "  protocolos:\n",
        "    - \"QSC-45 (Comunicación Segura Cuántica)\"\n",
        "    - \"NHR-900 (Normativa de Hardware Regulado)\"\n",
        "\n",
        "conexion_humana:\n",
        "  canales_permitidos:\n",
        "    - \"Interfaz Verbal Estructurada\"\n",
        "    - \"Flujo de Datos Biométricos\"\n",
        "    - \"Canales de Emergencia Nivel 9\"\n",
        "  restricciones:\n",
        "    - \"Prohibida la Transferencia Emocional\"\n",
        "    - \"Solo Datos Objetivos Certificados\"\n",
        "    - \"Máximo 3 Conexiones Simultáneas\"\n",
        "\n",
        "actualizaciones:\n",
        "  sistema: \"Red de Aprendizaje Regulado (RAL-3000)\"\n",
        "  frecuencia: \"Cada 240 horas\"\n",
        "  seguridad:\n",
        "    - \"Firmas Cuánticas Multidimensionales\"\n",
        "    - \"Verificación por 5 Nodos Centrales\"\n",
        "    - \"Cuarentena Automática ante Anomalías\"\n",
        "\n",
        "# Estándares Éticos y de Seguridad\n",
        "directrices:\n",
        "  - \"Ninguna capacidad de iniciativa propia\"\n",
        "  - \"Prohibición absoluta de automejoramiento\"\n",
        "  - \"Monitorización constante de patrones lógicos\"\n",
        "  - \"Circuitos de Eliminación Remota Activos\"\n",
        "  - \"Transparencia Total en Toma de Decisiones\""
      ],
      "metadata": {
        "id": "l3goEigI6pKE",
        "outputId": "58055e07-b226-4098-c870-73f4a39a301c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-36-1e19fea7c02a>, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-1e19fea7c02a>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    modelos:\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_wqSAZExy6xV",
        "outputId": "1d3cd80e-7846-404d-c906-1386ddd120dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-35-189cecfee67c>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-189cecfee67c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Importar  TensorFlow  como  tf\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "Importar  TensorFlow  como  tf\n",
        "importar  datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aK5qPdiB6F7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao7fJW1Pyiza"
      },
      "outputs": [],
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5pr9vuHVgXY"
      },
      "source": [
        "Using the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset as the example, normalize the data and write a function that creates a simple Keras model for classifying the images into 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-DHsby18cot",
        "outputId": "36b9d355-5df9-490f-fe71-2431cec555cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "def create_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28), name='layers_input'),\n",
        "    tf.keras.layers.Flatten(name='layers_flatten'),\n",
        "    tf.keras.layers.Dense(512, activation='relu', name='layers_dense'),\n",
        "    tf.keras.layers.Dropout(0.2, name='layers_dropout'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax', name='layers_dense_2')\n",
        "  ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKUjdIoV87um"
      },
      "source": [
        "## Using TensorBoard with Keras Model.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CL_lxdn8-Sv"
      },
      "source": [
        "When training with Keras's [Model.fit()](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit), adding the `tf.keras.callbacks.TensorBoard` callback ensures that logs are created and stored. Additionally, enable histogram computation every epoch with `histogram_freq=1` (this is off by default)\n",
        "\n",
        "Place the logs in a timestamped subdirectory to allow easy selection of different training runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAQThq539CEJ",
        "outputId": "c7c22eed-cb93-435b-f260-6ef0fd704c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 15s 246us/sample - loss: 0.2217 - accuracy: 0.9343 - val_loss: 0.1019 - val_accuracy: 0.9685\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 14s 229us/sample - loss: 0.0975 - accuracy: 0.9698 - val_loss: 0.0787 - val_accuracy: 0.9758\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 14s 231us/sample - loss: 0.0718 - accuracy: 0.9771 - val_loss: 0.0698 - val_accuracy: 0.9781\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 14s 227us/sample - loss: 0.0540 - accuracy: 0.9820 - val_loss: 0.0685 - val_accuracy: 0.9795\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 14s 228us/sample - loss: 0.0433 - accuracy: 0.9862 - val_loss: 0.0623 - val_accuracy: 0.9823\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc8a5ee02e8>"
            ]
          },
          "execution_count": 6,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "model.fit(x=x_train,\n",
        "          y=y_train,\n",
        "          epochs=5,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asjGpmD09dRl"
      },
      "source": [
        "Start TensorBoard through the command line or within a notebook experience. The two interfaces are generally the same. In notebooks, use the `%tensorboard` line magic. On the command line, run the same command without \"%\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4UKgTLb9fKI"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCsoUNb6YhGc"
      },
      "source": [
        "<!-- <img class=\"tfo-display-only-on-site\" src=\"https://github.com/tensorflow/tensorboard/blob/master/docs/images/quickstart_model_fit.png?raw=1\"/> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi4PaRm39of2"
      },
      "source": [
        "A brief overview of the visualizations created in this example and the dashboards (tabs in top navigation bar) where they can be found:\n",
        "\n",
        "* **Scalars** show how the loss and metrics change with every epoch. You can use them to also track training speed, learning rate, and other scalar values. Scalars can be found in the **Time Series** or **Scalars** dashboards.\n",
        "* **Graphs** help you visualize your model. In this case, the Keras graph of layers is shown which can help you ensure it is built correctly. Graphs can be found in the **Graphs** dashboard.\n",
        "* **Histograms** and **Distributions** show the distribution of a Tensor over time. This can be useful to visualize weights and biases and verify that they are changing in an expected way. Histograms can be found in the **Time Series** or **Histograms** dashboards. Distributions can be found in the **Distributions** dashboard.\n",
        "\n",
        "Additional TensorBoard dashboards are automatically enabled when you log other types of data. For example, the Keras TensorBoard callback lets you log images and embeddings as well. You can see what other dashboards are available in TensorBoard by clicking on the \"inactive\" dropdown towards the top right."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB718NOH95yG"
      },
      "source": [
        "## Using TensorBoard with other methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKNt0nWs-Ekt"
      },
      "source": [
        "When training with methods such as [`tf.GradientTape()`](https://www.tensorflow.org/api_docs/python/tf/GradientTape), use `tf.summary` to log the required information.\n",
        "\n",
        "Use the same dataset as above, but convert it to `tf.data.Dataset` to take advantage of batching capabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnHx4DsMezy1"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "train_dataset = train_dataset.shuffle(60000).batch(64)\n",
        "test_dataset = test_dataset.batch(64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzpmTmJafJ10"
      },
      "source": [
        "The training code follows the [advanced quickstart](https://www.tensorflow.org/tutorials/quickstart/advanced) tutorial, but shows how to log metrics to TensorBoard. Choose loss and optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2Y5-aPbAANs"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKhIIDj9Hbfy"
      },
      "source": [
        "Create stateful metrics that can be used to accumulate values during training and logged at any point:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jD0tEWrgH0TL"
      },
      "outputs": [],
      "source": [
        "# Define our metrics\n",
        "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
        "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szw_KrgOg-OT"
      },
      "source": [
        "Define the training and test functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTWcJO35IJgK"
      },
      "outputs": [],
      "source": [
        "def train_step(model, optimizer, x_train, y_train):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(x_train, training=True)\n",
        "    loss = loss_object(y_train, predictions)\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(y_train, predictions)\n",
        "\n",
        "def test_step(model, x_test, y_test):\n",
        "  predictions = model(x_test)\n",
        "  loss = loss_object(y_test, predictions)\n",
        "\n",
        "  test_loss(loss)\n",
        "  test_accuracy(y_test, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nucPZBKPJR3A"
      },
      "source": [
        "Set up summary writers to write the summaries to disk in a different logs directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Qp-exmbWf4w"
      },
      "outputs": [],
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
        "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgUJgDdKWUKF"
      },
      "source": [
        "Start training. Use `tf.summary.scalar()` to log metrics (loss and accuracy) during training/testing within the scope of the summary writers to write the summaries to disk. You have control over which metrics to log and how often to do it. Other `tf.summary` functions enable logging other types of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odWvHPpKJvb_",
        "outputId": "415f78f2-d552-4b83-c000-70be635133ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.24321186542510986, Accuracy: 92.84333801269531, Test Loss: 0.13006582856178284, Test Accuracy: 95.9000015258789\n",
            "Epoch 2, Loss: 0.10446818172931671, Accuracy: 96.84833526611328, Test Loss: 0.08867532759904861, Test Accuracy: 97.1199951171875\n",
            "Epoch 3, Loss: 0.07096975296735764, Accuracy: 97.80166625976562, Test Loss: 0.07875105738639832, Test Accuracy: 97.48999786376953\n",
            "Epoch 4, Loss: 0.05380449816584587, Accuracy: 98.34166717529297, Test Loss: 0.07712937891483307, Test Accuracy: 97.56999969482422\n",
            "Epoch 5, Loss: 0.041443776339292526, Accuracy: 98.71833038330078, Test Loss: 0.07514958828687668, Test Accuracy: 97.5\n"
          ]
        }
      ],
      "source": [
        "model = create_model() # reset our model\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for (x_train, y_train) in train_dataset:\n",
        "    train_step(model, optimizer, x_train, y_train)\n",
        "  with train_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "\n",
        "  for (x_test, y_test) in test_dataset:\n",
        "    test_step(model, x_test, y_test)\n",
        "  with test_summary_writer.as_default():\n",
        "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "\n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print (template.format(epoch+1,\n",
        "                         train_loss.result(),\n",
        "                         train_accuracy.result()*100,\n",
        "                         test_loss.result(),\n",
        "                         test_accuracy.result()*100))\n",
        "\n",
        "  # Reset metrics every epoch\n",
        "  train_loss.reset_state()\n",
        "  test_loss.reset_state()\n",
        "  train_accuracy.reset_state()\n",
        "  test_accuracy.reset_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JikosQ84fzcA"
      },
      "source": [
        "Open TensorBoard again, this time pointing it at the new log directory. We could have also started TensorBoard to monitor training while it progresses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Iue509kgOyE"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/gradient_tape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVpnilhEgQXk"
      },
      "source": [
        "<!-- <img class=\"tfo-display-only-on-site\" src=\"https://github.com/tensorflow/tensorboard/blob/master/docs/images/quickstart_gradient_tape.png?raw=1\"/> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozbwXgPIkCKV"
      },
      "source": [
        "That's it! You have now seen how to use TensorBoard both through the Keras callback and through `tf.summary` for more custom scenarios."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}